{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18cf1f2-cf37-49c0-ae3a-1127267c16d8",
   "metadata": {},
   "source": [
    "# A Customer Churn Prediction\n",
    "### Project Scenario\n",
    "Every company wants to increase its profit or revenue margin and customer retention is one key area industry players focus their resources. In today's world of machine learning, most companies build classification models to perform churn analysis on their customers. \n",
    "\n",
    "Classification in machine learning and statistics entails a supervised learning approach where the computer program learns from provided data to make new observations or classifications. The primary objective is to determine the class or category into which new data points will fall. In this project scenario, an elaborate analysis will be conducted to train at least four models for predicting customer churn in a telecom company. This analysis will adhere to the **CRISP-DM framework**, ensuring a structured and systematic approach to model development and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66896bd6",
   "metadata": {},
   "source": [
    "##### In this project I will be using the CRISP-DM Frame work for analysis\n",
    "## Step 1- Business Understanding\n",
    "1. Objectives:\n",
    "- Thoroughly understand the business perspective of customer retention and increasing profit/revenue margins.\n",
    "- Define specific objectives such as reducing customer churn rate by a certain percentage or increasing customer lifetime value.\n",
    "\n",
    "2. Assess the Situation:\n",
    "- Evaluate resource availability including human resources, data availability, computing resources, and budget constraints.\n",
    "- Identify project requirements such as data quality, data sources, and any legal or ethical considerations.\n",
    "- Assess risks and contingencies related to data privacy, model accuracy, and implementation challenges.\n",
    "- Conduct a cost-benefit analysis to understand the potential return on investment (ROI) of the project.\n",
    "\n",
    "3. Determine Data Mining Goals:\n",
    "- Define technical goals such as building a classification model to predict customer churn with high accuracy.\n",
    "- Specify success criteria for the data mining process, including model performance metrics (e.g., accuracy, precision, recall) and business impact metrics (e.g., cost savings, revenue increase).\n",
    "\n",
    "4. Produce Project Plan:\n",
    "- Select appropriate technologies and tools for data collection, preprocessing, modeling, and evaluation (e.g., Python for data analysis, scikit-learn for machine learning, etc.).\n",
    "- Define a detailed project plan with timelines, milestones, and responsibilities for each phase of the project (e.g., data collection, data preprocessing, model training, model evaluation, deployment).\n",
    "- Ensure alignment with stakeholders and obtain necessary approvals for the project plan.\n",
    "\n",
    "### Hypothesis\n",
    "Hypothesis 1\n",
    "Null Hypothesis (Ho): Customers with longer tenure (number of months stayed with the company) are less likely to churn compared to new customers.\n",
    "Alternative Hypothesis (Ha): Customers with longer tenure (number of months stayed with the company) are more likely to churn compared to new customers.\n",
    "\n",
    "Hypothesis 2\n",
    "Null Hypothesis (Ho): Customers with higher monthly charges (MonthlyCharges) are more likely to churn due to cost considerations.\n",
    "Alternative Hypothesis (Ha): Customers with higher monthly charges (MonthlyCharges) are less likely to churn due to cost considerations.\n",
    "\n",
    "### Business Questions\n",
    "1. What is the average tenure of customers who churned compared to those who stayed?\n",
    "2. Do customers with partners or dependents have a lower churn rate?\n",
    "3. How does the presence of multiple lines affect customer churn?\n",
    "4. Is there a correlation between the contract term (Contract) and customer churn?\n",
    "5. What are the common payment methods (Payment Method) among customers who churned?\n",
    "6. Are customers with higher monthly charges more likely to churn?\n",
    "7. How does the availability of tech-related services (e.g., OnlineSecurity, TechSupport) impact churn rates?\n",
    "8. What percentage of customers who churned had streaming services (StreamingTV, StreamingMovies)?\n",
    "9. Is there a difference in churn rates between senior citizens and non-senior citizens?\n",
    "10. How does the total amount charged to customers (TotalCharges) correlate with churn behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03371a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›¬ Imported all packages. Warnings hidden. ðŸ‘»\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values \n",
    "import pyodbc \n",
    "import numpy as np\n",
    "import pandas as pd                      \n",
    "import re     \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import statistics as stat \n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy import stats            \n",
    "from scipy.stats import ttest_ind  \n",
    "from matplotlib.ticker import FuncFormatter  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ›¬ Imported all packages.\", \"Warnings hidden. ðŸ‘»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64fae70",
   "metadata": {},
   "source": [
    "## Step 2 - Data Understanding\n",
    "The data for this project is in differnt files and will be loaded into the notebook. The following describes the columns present in the data.\n",
    "\n",
    "Gender -- Whether the customer is a male or a female\n",
    "\n",
    "SeniorCitizen -- Whether a customer is a senior citizen or not\n",
    "\n",
    "Partner -- Whether the customer has a partner or not (Yes, No)\n",
    "\n",
    "Dependents -- Whether the customer has dependents or not (Yes, No)\n",
    "\n",
    "Tenure -- Number of months the customer has stayed with the company\n",
    "\n",
    "Phone Service -- Whether the customer has a phone service or not (Yes, No)\n",
    "\n",
    "MultipleLines -- Whether the customer has multiple lines or not\n",
    "\n",
    "InternetService -- Customer's internet service provider (DSL, Fiber Optic, No)\n",
    "\n",
    "OnlineSecurity -- Whether the customer has online security or not (Yes, No, No Internet)\n",
    "\n",
    "OnlineBackup -- Whether the customer has online backup or not (Yes, No, No Internet)\n",
    "\n",
    "DeviceProtection -- Whether the customer has device protection or not (Yes, No, No internet service)\n",
    "\n",
    "TechSupport -- Whether the customer has tech support or not (Yes, No, No internet)\n",
    "\n",
    "StreamingTV -- Whether the customer has streaming TV or not (Yes, No, No internet service)\n",
    "\n",
    "StreamingMovies -- Whether the customer has streaming movies or not (Yes, No, No Internet service)\n",
    "\n",
    "Contract -- The contract term of the customer (Month-to-Month, One year, Two year)\n",
    "\n",
    "PaperlessBilling -- Whether the customer has paperless billing or not (Yes, No)\n",
    "\n",
    "Payment Method -- The customer's payment method (Electronic check, mailed check, Bank transfer(automatic), Credit card(automatic))\n",
    "\n",
    "MonthlyCharges -- The amount charged to the customer monthly\n",
    "\n",
    "TotalCharges -- The total amount charged to the customer\n",
    "\n",
    "Churn -- Whether the customer churned or not (Yes or No)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39d5c1",
   "metadata": {},
   "source": [
    "#### First Dataset\n",
    "I'm using Python's dotenv with a .env file to safely fetch the first dataset from a SQL database into my notebook. This keeps my database credentials private while allowing easy access to the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22785d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40a7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection string\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};MARS_Connection=yes;MinProtocolVersion=TLSv1.2;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546d467d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53); [08001] [Microsoft][ODBC SQL Server Driver]Invalid connection string attribute (0)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the connect method of the pyodbc library and pass in the connection string.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpyodbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOperationalError\u001b[0m: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53); [08001] [Microsoft][ODBC SQL Server Driver]Invalid connection string attribute (0)')"
     ]
    }
   ],
   "source": [
    "# Use the connect method of the pyodbc library and pass in the connection string.\n",
    "\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa39f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sql query to get the 2020 data\n",
    "query = \"Select * FROM LP2_Telco_churn_first_3000\"\n",
    "First_Dataset = pd.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75982cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the first Data set \n",
    "First_Dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "First_Dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945ba02",
   "metadata": {},
   "source": [
    "#### Second Dataset\n",
    "I obtained the second dataset from a GitHub repository, and I'll use Pandas to import the CSV file into my notebook for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91159fef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'My data\\\\LP2_Telco-churn-second-2000.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the second dataset into the notebook\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Second_Dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMy data\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mLP2_Telco-churn-second-2000.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\AZUBI CLASS\\Career Accelerator\\LP2\\A-Customer-Churn-Prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\AZUBI CLASS\\Career Accelerator\\LP2\\A-Customer-Churn-Prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Desktop\\AZUBI CLASS\\Career Accelerator\\LP2\\A-Customer-Churn-Prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\AZUBI CLASS\\Career Accelerator\\LP2\\A-Customer-Churn-Prediction\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Desktop\\AZUBI CLASS\\Career Accelerator\\LP2\\A-Customer-Churn-Prediction\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'My data\\\\LP2_Telco-churn-second-2000.csv'"
     ]
    }
   ],
   "source": [
    "# Loading the second dataset into the notebook\n",
    "Second_Dataset = pd.read_csv(r\"My data\\LP2_Telco-churn-second-2000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second_Dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "Second_Dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e32232",
   "metadata": {},
   "source": [
    "#### Third Dataset\n",
    "I obtained the third dataset from a OneDrive file, and I will use Pandas to import the Excel file into the notebook for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the second dataset into the notebook\n",
    "Third_Dataset = pd.read_excel(r\"My data\\Telco-churn-last-2000.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d179b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "Third_Dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Third_Dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the first and second Dataset have the same column names for easy concatenation\n",
    "# Get the column names from each DataFrame\n",
    "columns_First_Dataset = set(First_Dataset.columns)\n",
    "columns_Second_Dataset = set(Second_Dataset.columns)\n",
    "\n",
    "# Check if all DataFrames have the same column names\n",
    "if columns_First_Dataset == columns_Second_Dataset:\n",
    "    print(\"All DataFrames have the same column names.\")\n",
    "else:\n",
    "    print(\"Not all DataFrames have the same column names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390681cf",
   "metadata": {},
   "source": [
    "Concatenating the first and second Dataset to prepare them for analysis and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([First_Dataset, Second_Dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e313071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column names to lowercase\n",
    "train_df.columns = train_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bceb7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11eb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include =\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f347041",
   "metadata": {},
   "source": [
    "### Univariate Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_df, grid=False, color=\"lightgreen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d35824",
   "metadata": {},
   "source": [
    "### Bivariate Analysis:\n",
    "Gender vs. Churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eabacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis: Gender vs. Churn\n",
    "sns.countplot(data=train_df, x='gender', hue='churn')\n",
    "plt.title('Churn Rate by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Churn', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2f09d",
   "metadata": {},
   "source": [
    "SeniorCitizen vs. MonthlyCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c61fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis: SeniorCitizen vs. MonthlyCharges\n",
    "sns.boxplot(data=train_df, x='seniorcitizen', y='monthlycharges')\n",
    "plt.title('Monthly Charges by Senior Citizen Status')\n",
    "plt.xlabel('Senior Citizen')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7fecb",
   "metadata": {},
   "source": [
    "Partner vs. Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis: Partner vs. Tenure\n",
    "sns.barplot(data=train_df, x='partner', y='tenure', estimator=sum)\n",
    "plt.title('Total Tenure by Partner Status')\n",
    "plt.xlabel('Partner')\n",
    "plt.ylabel('Total Tenure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5bfb7",
   "metadata": {},
   "source": [
    "### Multivariate Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ea1d3",
   "metadata": {},
   "source": [
    "Gender, SeniorCitizen vs. Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b419f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate analysis: Gender, SeniorCitizen vs. Churn\n",
    "sns.catplot(data=train_df, x='gender', hue='churn', col='seniorcitizen', kind='count')\n",
    "plt.suptitle('Churn Rate by Gender and Senior Citizen Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fe33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_columns = train_df.select_dtypes(include=['number']).columns\n",
    "categorical_columns = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Correlation Matrix and Heatmap for Numeric Variables\n",
    "numeric_df = train_df[numeric_columns]\n",
    "numeric_correlation_matrix = numeric_df.corr()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(numeric_correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix Heatmap (Numeric Variables)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b08235",
   "metadata": {},
   "source": [
    "InternetService, OnlineSecurity vs. MonthlyCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate analysis: InternetService, OnlineSecurity vs. MonthlyCharges\n",
    "sns.boxplot(data=train_df, x='internetservice', y='monthlycharges', hue='onlinesecurity')\n",
    "plt.title('Monthly Charges by Internet Service and Online Security')\n",
    "plt.xlabel('Internet Service')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.legend(title='Online Security', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624180e",
   "metadata": {},
   "source": [
    "Contract, PaperlessBilling vs. Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate analysis: Contract, PaperlessBilling vs. Tenure\n",
    "sns.barplot(data=train_df, x='contract', y='tenure', hue='paperlessbilling', estimator=sum)\n",
    "plt.title('Total Tenure by Contract and Paperless Billing')\n",
    "plt.xlabel('Contract')\n",
    "plt.ylabel('Total Tenure')\n",
    "plt.legend(title='Paperless Billing', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for uniques\n",
    "cols= [ 'customerid', 'gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'tenure', 'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod', 'monthlycharges', 'totalcharges', 'churn']\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in cols:\n",
    "    \n",
    "    unique_values = train_df[i].unique()\n",
    "    num_unique_values = train_df[i].nunique()\n",
    "    \n",
    "    \n",
    "    results.append([i, unique_values, num_unique_values])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Column', 'Unique_Values', 'Num_Unique_Values'])\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23318b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for standardization\n",
    "standardize_mapping = {\n",
    "    'No phone service': 'No',\n",
    "    'No internet service': 'No',\n",
    "    'True': 'Yes',\n",
    "    'False': 'No',\n",
    "    'None': pd.NA,\n",
    "    'none': pd.NA\n",
    "}\n",
    "\n",
    "# Columns to perform standardization\n",
    "cols_to_standardize = ['onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "                       'streamingtv', 'streamingmovies', 'paperlessbilling', 'churn',\n",
    "                       'phoneservice', 'multiplelines', 'partner', 'dependents']\n",
    "\n",
    "# Loop through columns for standardization\n",
    "for col in cols_to_standardize:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        train_df[col] = train_df[col].replace(standardize_mapping)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce8d88",
   "metadata": {},
   "source": [
    "#### Key Insights\n",
    "1. I observed that many categorical columns contain values such as \"NO,\" \"Yes,\" \"False,\" and \"True.\" I will update all \"True\" values to \"Yes\" and all \"False\" values to \"No.\"\n",
    "2. I standardized the case of all columns in the DataFrame to lowercase letters for uniformity.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
